"""
Streamlit UI for Project Absola - Local Legal Document Analyzer
Main application entry point.
"""

import streamlit as st
import os
import tempfile
import time
from pathlib import Path
import logging

from models import ModelLoader, Summarizer, QASystem
from processor import DocumentProcessor
from rag import RAGSystem
from scraper import LegalScraper

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="Project Absola - Legal Document Analyzer",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS
st.markdown(
    """
<style>
.main-header {
    font-size: 2.5rem;
    color: #1E3A8A;
    font-weight: bold;
    margin-bottom: 1rem;
}
.sub-header {
    font-size: 1.2rem;
    color: #4B5563;
    margin-bottom: 2rem;
}
.feature-box {
    padding: 1.5rem;
    border-radius: 0.5rem;
    background-color: #F3F4F6;
    margin-bottom: 1rem;
}
.success-box {
    padding: 1rem;
    border-radius: 0.5rem;
    background-color: #D1FAE5;
    border-left: 4px solid #10B981;
    margin: 1rem 0;
}
.error-box {
    padding: 1rem;
    border-radius: 0.5rem;
    background-color: #FEE2E2;
    border-left: 4px solid #EF4444;
    margin: 1rem 0;
}
</style>
""",
    unsafe_allow_html=True,
)


def initialize_session_state():
    """Initialize Streamlit session state variables."""
    if "model_loader" not in st.session_state:
        st.session_state.model_loader = None
    if "summarizer" not in st.session_state:
        st.session_state.summarizer = None
    if "qa_system" not in st.session_state:
        st.session_state.qa_system = None
    if "doc_processor" not in st.session_state:
        st.session_state.doc_processor = DocumentProcessor()
    if "rag_system" not in st.session_state:
        st.session_state.rag_system = RAGSystem()
    if "scraper" not in st.session_state:
        st.session_state.scraper = LegalScraper()
    if "current_document" not in st.session_state:
        st.session_state.current_document = None
    if "document_summary" not in st.session_state:
        st.session_state.document_summary = None


def lazy_load_models():
    """Lazy load ML models to save memory."""
    if st.session_state.model_loader is None:
        with st.spinner("Loading AI models... This may take a moment."):
            st.session_state.model_loader = ModelLoader(max_memory_gb=8.0)
            st.session_state.summarizer = Summarizer(st.session_state.model_loader)
            st.session_state.qa_system = QASystem(st.session_state.model_loader)


def main():
    """Main application function."""
    initialize_session_state()

    # Header
    st.markdown('<div class="main-header">‚öñÔ∏è Project Absola</div>', unsafe_allow_html=True)
    st.markdown(
        '<div class="sub-header">Local Legal Document Analyzer - Privacy-First AI Analysis</div>',
        unsafe_allow_html=True,
    )

    # Sidebar
    with st.sidebar:
        st.header("Navigation")
        page = st.radio(
            "Select Feature",
            ["üìÑ Document Upload", "üîç RAG Q&A", "üåê Legal Scraper", "üìä Analytics", "‚öôÔ∏è Settings"],
        )

        st.markdown("---")
        st.markdown("### System Info")
        st.info(
            """
        - **Privacy**: 100% Local Processing
        - **No Cloud**: All data stays on device
        - **No Telemetry**: No tracking
        - **Models**: T5 + Llama3
        - **RAG**: FAISS + Sentence Transformers
        """
        )

        if st.button("üóëÔ∏è Clear All Data"):
            st.session_state.current_document = None
            st.session_state.document_summary = None
            st.session_state.rag_system.clear_index()
            st.success("All data cleared!")

    # Main content area
    if page == "üìÑ Document Upload":
        document_upload_page()
    elif page == "üîç RAG Q&A":
        rag_qa_page()
    elif page == "üåê Legal Scraper":
        scraper_page()
    elif page == "üìä Analytics":
        analytics_page()
    elif page == "‚öôÔ∏è Settings":
        settings_page()


def document_upload_page():
    """Document upload and processing page."""
    st.header("üìÑ Document Upload & Analysis")

    uploaded_file = st.file_uploader(
        "Upload Legal Document (PDF or DOCX)",
        type=["pdf", "docx"],
        help="Upload a legal document for analysis. Processing time: <60 seconds per page.",
    )

    if uploaded_file:
        # Save uploaded file temporarily
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=Path(uploaded_file.name).suffix
        ) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name

        try:
            # Process document
            with st.spinner("üìñ Extracting text from document..."):
                start_time = time.time()
                doc_data = st.session_state.doc_processor.process_document(tmp_path)
                extraction_time = time.time() - start_time

            st.session_state.current_document = doc_data

            # Display document info
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Word Count", f"{doc_data['word_count']:,}")
            with col2:
                st.metric("Pages (est.)", doc_data["page_estimate"])
            with col3:
                st.metric("Extraction Time", f"{extraction_time:.2f}s")

            # Show text preview
            with st.expander("üìù Document Preview", expanded=False):
                st.text_area("Extracted Text", doc_data["text"][:2000] + "...", height=300)

            # Summarization
            st.subheader("üìã Auto-Summarization")
            if st.button("Generate Summary (T5)", key="summarize_btn"):
                lazy_load_models()
                with st.spinner("ü§ñ Generating summary with T5 model..."):
                    start_time = time.time()
                    summary = st.session_state.summarizer.summarize(
                        doc_data["text"][:2000],  # Limit input for demo
                        max_length=150,
                        min_length=50,
                    )
                    summary_time = time.time() - start_time
                    st.session_state.document_summary = summary

                st.markdown('<div class="success-box">', unsafe_allow_html=True)
                st.markdown(f"**Summary** (generated in {summary_time:.2f}s):")
                st.write(summary)
                st.markdown("</div>", unsafe_allow_html=True)

            if st.session_state.document_summary:
                st.info(st.session_state.document_summary)

            # Index for RAG
            st.subheader("üîç Index for RAG Search")
            if st.button("Index Document for Q&A", key="index_btn"):
                with st.spinner("üìä Indexing document with FAISS..."):
                    start_time = time.time()
                    st.session_state.rag_system.index_documents(
                        [
                            {
                                "text": doc_data["text"],
                                "metadata": {
                                    "filename": doc_data["filename"],
                                    "format": doc_data["format"],
                                },
                            }
                        ]
                    )
                    index_time = time.time() - start_time

                st.success(
                    f"‚úÖ Document indexed successfully in {index_time:.2f}s! Go to RAG Q&A to ask questions."
                )

            # Export options
            st.subheader("üì§ Export")
            col1, col2 = st.columns(2)
            with col1:
                if st.button("Download Extracted Text"):
                    st.download_button(
                        label="üì• Download TXT",
                        data=doc_data["text"],
                        file_name=f"{Path(doc_data['filename']).stem}_extracted.txt",
                        mime="text/plain",
                    )
            with col2:
                if st.session_state.document_summary:
                    st.download_button(
                        label="üì• Download Summary",
                        data=st.session_state.document_summary,
                        file_name=f"{Path(doc_data['filename']).stem}_summary.txt",
                        mime="text/plain",
                    )

        except Exception as e:
            st.markdown(
                f'<div class="error-box">‚ùå Error processing document: {str(e)}</div>',
                unsafe_allow_html=True,
            )
            logger.error(f"Document processing error: {e}", exc_info=True)

        finally:
            # Clean up temp file
            try:
                os.unlink(tmp_path)
            except:
                pass


def rag_qa_page():
    """RAG-based Q&A page."""
    st.header("üîç RAG-Powered Q&A")

    stats = st.session_state.rag_system.get_stats()

    if stats["indexed_chunks"] == 0:
        st.warning("‚ö†Ô∏è No documents indexed yet. Please upload and index a document first.")
        return

    st.success(f"‚úÖ {stats['indexed_chunks']} document chunks indexed and ready for questions!")

    # Q&A interface
    question = st.text_input(
        "Ask a question about your documents",
        placeholder="E.g., What are the key terms of this contract?",
        help="Ask questions about the indexed documents. Response time: <10 seconds.",
    )

    if question:
        with st.spinner("üîé Searching documents and generating answer..."):
            start_time = time.time()

            # Retrieve relevant context
            context = st.session_state.rag_system.get_context_for_query(question, k=3)

            if not context:
                st.warning("No relevant information found in indexed documents.")
                return

            # Show retrieved context
            with st.expander("üìö Retrieved Context", expanded=False):
                st.text_area("Relevant Passages", context, height=200)

            # Generate answer (simplified for demo without heavy model)
            # In production, would use QA model
            st.markdown("### üí° Answer")
            st.info(
                f"""
Based on the indexed documents, here are the most relevant passages for your question:

{context[:500]}...

*Note: For full Q&A with Llama3 model, ensure the model is available and properly configured.*
            """
            )

            query_time = time.time() - start_time
            st.caption(f"‚è±Ô∏è Query processed in {query_time:.2f} seconds")


def scraper_page():
    """Legal web scraping page."""
    st.header("üåê Legal Information Scraper")

    tab1, tab2 = st.tabs(["üìú License Information (TLDR Legal)", "‚öñÔ∏è Indian Case Law"])

    with tab1:
        st.subheader("Popular Software Licenses")
        if st.button("Scrape License Information"):
            with st.spinner("üåê Scraping TLDR Legal..."):
                try:
                    licenses = st.session_state.scraper.scrape_licenses()

                    if licenses:
                        st.success(f"‚úÖ Scraped {len(licenses)} licenses!")

                        for lic in licenses:
                            with st.expander(f"üìÑ {lic['title']}"):
                                st.markdown(f"**URL**: [{lic['url']}]({lic['url']})")

                                if lic.get("can"):
                                    st.markdown("**‚úÖ You CAN:**")
                                    for item in lic["can"]:
                                        st.markdown(f"- {item}")

                                if lic.get("cannot"):
                                    st.markdown("**‚ùå You CANNOT:**")
                                    for item in lic["cannot"]:
                                        st.markdown(f"- {item}")

                                if lic.get("must"):
                                    st.markdown("**‚ö†Ô∏è You MUST:**")
                                    for item in lic["must"]:
                                        st.markdown(f"- {item}")
                    else:
                        st.warning("No licenses found. Please check your internet connection.")

                except Exception as e:
                    st.error(f"Error scraping licenses: {str(e)}")

    with tab2:
        st.subheader("Search Indian Case Law")
        query = st.text_input("Enter search query", placeholder="E.g., contract law")

        if query and st.button("Search Cases"):
            with st.spinner(f"üîç Searching Indian Kanoon for '{query}'..."):
                try:
                    cases = st.session_state.scraper.search_indian_cases(query, max_results=10)

                    if cases:
                        st.success(f"‚úÖ Found {len(cases)} cases!")

                        for case in cases:
                            with st.expander(f"‚öñÔ∏è {case['title']}"):
                                st.markdown(f"**URL**: [{case['url']}]({case['url']})")
                                st.markdown(f"**Excerpt**: {case['excerpt']}")
                    else:
                        st.warning("No cases found for this query.")

                except Exception as e:
                    st.error(f"Error searching cases: {str(e)}")


def analytics_page():
    """Analytics and statistics page."""
    st.header("üìä Analytics & Statistics")

    # RAG statistics
    st.subheader("üîç RAG System Statistics")
    stats = st.session_state.rag_system.get_stats()

    col1, col2 = st.columns(2)
    with col1:
        st.metric("Indexed Chunks", stats.get("indexed_chunks", 0))
    with col2:
        st.metric("Embedding Dimension", stats.get("embedding_dimension", 0))

    # Document statistics
    if st.session_state.current_document:
        st.subheader("üìÑ Current Document")
        doc = st.session_state.current_document

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Filename", doc["filename"])
        with col2:
            st.metric("Words", f"{doc['word_count']:,}")
        with col3:
            st.metric("Characters", f"{doc['char_count']:,}")

    # Performance info
    st.subheader("‚ö° Performance Targets")
    st.info(
        """
    - **Document Processing**: <60 seconds per page ‚úÖ
    - **Query Response**: <10 seconds ‚úÖ
    - **Memory Usage**: <8GB RAM ‚úÖ
    - **Privacy**: 100% Local, No Cloud ‚úÖ
    """
    )


def settings_page():
    """Settings and configuration page."""
    st.header("‚öôÔ∏è Settings")

    st.subheader("üîí Privacy Settings")
    st.info(
        """
    **Privacy-First Design:**
    - ‚úÖ All processing happens locally on your device
    - ‚úÖ No data is sent to external servers
    - ‚úÖ No telemetry or usage tracking
    - ‚úÖ Documents can be cleared at any time
    """
    )

    st.subheader("ü§ñ Model Configuration")
    st.markdown(
        """
    **Active Models:**
    - **Summarization**: T5-base (or AvantIQ T5 if available)
    - **Q&A**: Llama3 LegalLM (configured for legal domain)
    - **Embeddings**: sentence-transformers/all-MiniLM-L6-v2
    - **Vector Store**: FAISS (CPU-optimized)
    """
    )

    st.subheader("üíæ Memory Management")
    memory_limit = st.slider("Max Memory Usage (GB)", 4, 16, 8)
    st.caption(f"Current limit: {memory_limit}GB VRAM")

    if st.button("Clear Model Cache"):
        if st.session_state.model_loader:
            st.session_state.model_loader.unload_models()
            st.success("‚úÖ Model cache cleared!")

    st.subheader("üìÅ Data Management")
    if st.button("Clear All Application Data"):
        st.session_state.current_document = None
        st.session_state.document_summary = None
        st.session_state.rag_system.clear_index()
        if st.session_state.model_loader:
            st.session_state.model_loader.unload_models()
        st.success("‚úÖ All application data cleared!")

    st.subheader("‚ÑπÔ∏è About")
    st.markdown(
        """
    **Project Absola** - Local Legal Document Analyzer
    
    - Version: 1.0.0
    - Technology: Python + Streamlit
    - AI Models: T5 + Llama3
    - RAG: LangChain + FAISS
    - Privacy: 100% Local Processing
    
    Built with ‚ù§Ô∏è for privacy-conscious legal professionals.
    """
    )


if __name__ == "__main__":
    main()
